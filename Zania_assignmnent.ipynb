{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aa63f561",
      "metadata": {
        "id": "aa63f561"
      },
      "source": [
        "## Install the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "HzWXr2z0bFYX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzWXr2z0bFYX",
        "outputId": "72c0c99e-98e5-48ac-8d07-05d36ee42ea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.35.8)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "!pip install tiktoken\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NzBReQ1w6VBO",
      "metadata": {
        "id": "NzBReQ1w6VBO"
      },
      "source": [
        "#Import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "qXB7x5T5fSMR",
      "metadata": {
        "id": "qXB7x5T5fSMR"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "import re\n",
        "import tiktoken\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "qVPhq7oHbFVF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVPhq7oHbFVF",
        "outputId": "139d905c-c3e1-4064-b76e-c63b3bb8d1b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.35.8\n"
          ]
        }
      ],
      "source": [
        "print(openai.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MqXRaG666bUE",
      "metadata": {
        "id": "MqXRaG666bUE"
      },
      "source": [
        "#Load the pre-trained transformer model. This model I have used to get embeddings of each page and the given question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "hmTwV_1EVtw0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmTwV_1EVtw0",
        "outputId": "487b5553-86ed-4cac-e39f-ba62f3ad79c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load a pretrained Sentence Transformer model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NSqEbIA77DBC",
      "metadata": {
        "id": "NSqEbIA77DBC"
      },
      "source": [
        "# Define the secret key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "-MJns0XoTj_4",
      "metadata": {
        "id": "-MJns0XoTj_4"
      },
      "outputs": [],
      "source": [
        "# Provide the secret key to use the OpenAI API\n",
        "openai_secret_key = \"OPENAI-API-KEY\"\n",
        "client = openai.OpenAI(api_key= openai_secret_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "89af4c0d",
      "metadata": {
        "id": "89af4c0d"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    '''Function to replace next line with space'''\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    cleaned_text = text.replace(\"\\n\",\" \")\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "dhReIN9Yjvdm",
      "metadata": {
        "id": "dhReIN9Yjvdm"
      },
      "outputs": [],
      "source": [
        "def text_extraction_from_pdf(pdf_file_path):\n",
        "    '''Function to extract text from pdf'''\n",
        "    df = pd.DataFrame(columns = [\"Page_Number\", \"Segments\"])\n",
        "    reader = PyPDF2.PdfReader(pdf_file_path)\n",
        "    total_pages = len(reader.pages)\n",
        "\n",
        "    text_in_pages = []\n",
        "    for i in range(total_pages):\n",
        "        page = reader.pages[i]\n",
        "        text_in_pages.append(page.extract_text())\n",
        "\n",
        "    df[\"Segments\"] = text_in_pages\n",
        "    df['Segments'] = df[\"Segments\"].apply(lambda x : clean_text(x))\n",
        "\n",
        "    for j in range(total_pages):\n",
        "        df[\"Page_Number\"][j] = j+1\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "aaMThxGPYBSp",
      "metadata": {
        "id": "aaMThxGPYBSp"
      },
      "outputs": [],
      "source": [
        "def num_tokens(text, encoding_name= \"cl100k_base\"):\n",
        "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    return len(encoding.encode(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "U0_EV2A0YBPo",
      "metadata": {
        "id": "U0_EV2A0YBPo"
      },
      "outputs": [],
      "source": [
        "def search_docs(df, quest, top_n):\n",
        "    quest_embedding = model.encode(quest).reshape(1,-1)\n",
        "    df[\"similarities\"] = df.Embeddings.apply(lambda x: cosine_similarity(x.reshape(1,-1), quest_embedding))\n",
        "\n",
        "    res = df.sort_values(\"similarities\", ascending=False).head(top_n)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "h9u8N-MfYBMj",
      "metadata": {
        "id": "h9u8N-MfYBMj"
      },
      "outputs": [],
      "source": [
        "def query_message(quest,df,token_budget):\n",
        "    '''Function to get the pages that are relevant to the question.'''\n",
        "    question = f\"\\n\\nQuestion: {quest}\"\n",
        "    message = \"\"\n",
        "    for text in df['Segments']:\n",
        "        if (num_tokens(message + text + question)> token_budget):\n",
        "            break\n",
        "        else:\n",
        "            message += text\n",
        "    return message + question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "yAazaOoeYBKY",
      "metadata": {
        "id": "yAazaOoeYBKY"
      },
      "outputs": [],
      "source": [
        "def ask(quest,df,token_budget= 16385 - 500):\n",
        "    '''Function to ask the question and get the answer from the pages that are relevant to the question.'''\n",
        "    # Number of pages that are relevant to the question.\n",
        "    number_relevant_pages = 2\n",
        "    result = search_docs(df, quest, number_relevant_pages)\n",
        "    quest= quest + \" \" + \"Give the answer of the above question in 1-2 sentence only. Please don't provide extra information which is not mentioned in the question.\"\n",
        "    message = query_message(quest, result, token_budget=token_budget)\n",
        "    introduction = 'Use the following text only to answer the subsequent question. If the answer cannot be found in the provided text, write \"Data Not Available.\"'\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": introduction},\n",
        "        {\"role\": \"user\", \"content\": message},\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "        model= \"gpt-3.5-turbo-0125\",\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    response_message = response.choices[0].message.content\n",
        "    return response_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "jGxjz-NsJEk_",
      "metadata": {
        "id": "jGxjz-NsJEk_"
      },
      "outputs": [],
      "source": [
        "def get_answers_from_given_pdf_file(pdf_file_path, question):\n",
        "    df = text_extraction_from_pdf(pdf_file_path)\n",
        "    df['Embeddings'] = df[\"Segments\"].apply(lambda x : model.encode(x))\n",
        "    answer= ask(question, df)\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "0Oh5g15MmUSN",
      "metadata": {
        "id": "0Oh5g15MmUSN"
      },
      "outputs": [],
      "source": [
        "ques1 = \"What is the name of the company?\"\n",
        "ques2 = \"Who is the CEO of the company?\"\n",
        "ques3 = \"What is their vacation policy?\"\n",
        "ques4 = \"What is the termination policy?\"\n",
        "\n",
        "pdf_file_path = \"/content/handbook.pdf\"\n",
        "all_ques=[ques1, ques2, ques3, ques4]\n",
        "all_answers = {}\n",
        "for ques in all_ques:\n",
        "    answer= get_answers_from_given_pdf_file(pdf_file_path, ques)\n",
        "    all_answers[ques] = answer\n",
        "answer_in_json_form = json.dumps(all_answers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "qShYA7Lsv9yS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "qShYA7Lsv9yS",
        "outputId": "37ace1c0-073c-42b5-cc55-aaa1b44cb9f3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\"What is the name of the company?\": \"The name of the company is Zania, Inc.\", \"Who is the CEO of the company?\": \"The CEO of the company is Shruti Gupta.\", \"What is their vacation policy?\": \"Vacation is prorated based on hire date, employees accrue vacation up to a maximum amount, and unused vacation may be forfeited upon separation of employment.\", \"What is the termination policy?\": \"The termination policy at Zania, Inc. includes progressive discipline starting with verbal warnings, followed by written warnings, and potentially leading to demotion, transfer, forced leave, or termination of employment, depending on the circumstances.\"}'"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer_in_json_form"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
